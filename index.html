<html>
<meta charset="utf-8">
</head><title>Seminario:  2024 -2 </title></head>
<body bgcolor=white text=black link=#FF6600 alink=#0099CC 
vlink=#0099CC><font face="Trebuchet MS"><center>

<font size="10"> <b>Seminario: Optimización y aprendizaje automático</b><br> <p></p> 

<font size="5"> </center> 

<p><font size="6">El objetivo del seminario es discutir algunos puntos de vista vigentes sobre la relación entre modelos de optimización regularizada (tipo LASSO) y la teoría matemática de aprendizaje automático (machine learning). El seminario será autocontenido y no asume conocimientos previos ni de optimización ni de ML. </p>

<p><b>HORARIO: Jueves 14.30-16.00 en Salón 703-Rojo FIng.</b></p>
  
<center>
<p><b> CALENDARIO DEL SEMINARIO: </b></p>
<table border="1">
<tr>
<td> Fecha </td>  
<td> Titulo </td>
<td> Tema </td>
<td> Conferencista </td>
<td> Links</td>
<td> </td>
</tr>

<tr>
<td> 29/8 </td>  
<td> Métrica de Wasserstein y machine learning distribucionalmente robusto (DRO)</td>
<td> DRO </td>
<td> M.Velasco </td>
<td><a href="https://arxiv.org/abs/1908.08729"> [arXiv],<a href="https://www.youtube.com/watch?v=piYnd_wYlT8"> [Video]</td>
<td></td>
</tr>

<tr>
<td> 5/9 </td>     
<td> Suspendido </td>
<td>  </td>
<td> </td>
<td></td>
<td></td>
</tr>

<tr>
<td> 12/9 </td>   
<td> Construcción de Matrices para Compressive Sensing</td>
<td>  </td>
<td> P.Raigorodsky</td>
<td><a href=https://epubs.siam.org/doi/abs/10.1137/060657704> [Donoho, Bruckstein, Elad]</a></td>
<td></td>
</tr>

<tr>
<td> 19/9 </td>     
<td> Landscape de optimización cuando factorizamos matrices</td>
<td>  </td>
<td> M.Fiori </td>
<td><a href="https://arxiv.org/abs/2209.15130"> [arXiv]</a></td>
<td></td>
</tr>

<tr>
<td> 26/9 </td>     
<td> Minimización de rango mediante la norma nuclear</td>
<td>  </td>
<td> F.Carrasco </td>
<td><a href="https://arxiv.org/abs/0706.4138">[arXiv]</a></td>
<td></td>
</tr>  

<tr>
<td> 3/10 </td>     
<td> Universal priors for sparse coding</td>
<td>  </td>
<td> I.Ramírez </td>
<td><a href="https://arxiv.org/abs/1003.2941">[arXiv]</a></td>
<td></td>
</tr>  

<tr>
<td> 10/10 </td>     
<td> Brief history of denoising and their “nobel” conception as implicit manifold learners.</td>
<td>  </td>
<td> M. Di Martino </td>
<td><a href="https://www.fing.edu.uy/~mfiori/pdfs/charla_mdm_semOML.pdf">[slides]</a><a href="https://arxiv.org/pdf/2007.13640">[arXiv]</a>, <a href="https://arxiv.org/abs/2409.06219">[arXiv]</a></td>
<td></td>
</tr>  

<tr>
<td> 17/10 </td>     
<td> No hay seminario </td>
<td>  </td>
<td> </td>
<td></td>
<td></td>
</tr>  

<tr>
<td> 24/10 </td>     
<td> On the Benefits of Rank in Attention Layers</td>
<td>  </td>
<td> L. Raad </td>
<td><a href="https://arxiv.org/abs/2407.16153">[arXiv]</a></td>
<td></td>
</tr>  

<tr>
<td> 31/10 </td>     
<td> No hay seminario </td>
<td>  </td>
<td>  </td>
<td></td>
<td></td>
</tr>  
  
<tr>
<td> 7/11 </td>     
<td> Conformal Prediction </td>
<td>  </td>
<td> B. Marenco </td>
<td><a href="https://www.fing.edu.uy/~mfiori/pdfs/presentacion_conformal_handout.pdf">[slides]</a><a href="https://www.stat.berkeley.edu/~ryantibs/statlearn-s23/lectures/conformal.pdf"> [notas]</a></td>
<td></td>
</tr>    

<tr>
<td> 14/11 </td>     
<td> El Kernel de CD para Análisis de Datos </td>
<td> </td>
<td> L. Bentancur </td>
<td><a href="Seminario_OML_Bentancur.pdf">[slides]</td>
<td></td>
</tr>    

<tr>
<td> 21/11 </td>     
<td> Suspendido </td>
<td>  </td>
<td>  </td>
<td></td>
<td></td>
</tr>    

<tr>
<td> 28/11 </td>     
<td>  Defensa</td>
<td>  </td>
<td> B. Marenco </td>
<td></td>
<td></td>
</tr>    

<tr>
<td> 05/12 </td>     
<td> Deep Tempest </td>
<td>  </td>
<td> F. La Rocca </td>
<td></td>
<td></td>
</tr>    



<tr>
<td> 12/12 </td>     
<td> When the order matters: architectures for sequences</td>
<td> </td>
<td> Octavia Camps (Northeastern) </td>
<td></td>
<td></td>
</tr>    

<tr>
<td> 19/12 </td>     
<td> TBA</td>
<td> </td>
<td> Juan Cervino (MIT) </td>
<td></td>
<td></td>
</tr>    


  
</table>
</center>

<font size="5">
<p>ORGANIZADORES: 
<li>Pedro Raigorodsky (pedro.raigorodsky@gmail.com)</li> 
<li>Marcelo Fiori (mfiori@fing.edu.uy)</li> 
<li>Mauricio Velasco (mauricio.velasco@ucu.edu.uy)</li> 
</p>
<p>Si quiere dar una charla en el Seminario por favor escribir a los organizadores. Abajo hay algunas referencias con temas y artículos de posible interés (pero charlas de un tema distinto, en un área afin a la descripción del seminario estan bienvenidas!)</p>

<center>
<p><b>Referencias de interes:</b></p>
<table border="1">
<tr>
<td> Titulo </td>
<td> Tema </td>
<td> Link</td>
<td> EsClave</td>
</tr>



<tr>
<td> Compressive Sampling </td>
<td> Compressive Sensing  </td>
<td><a href="https://candes.su.domains/publications/downloads/CompressiveSampling.pdf"> [Link] </td>
<td>*</td>
</tr>

<tr>
<td> Compressive Fourier </td>
<td> Compressive Sensing  </td>
<td><a href="https://arxiv.org/pdf/math/0409186"> [arXiv]</td>
<td></td>
</tr>


<tr>
<td> CS on measures </td>
<td> Compressive Sensing  </td>
<td><a href="https://arxiv.org/pdf/1103.4951"> [arXiv]</td>
<td></td>
</tr>



<tr>
<td> Rank Minimization (RM) via nuclear norm regularization </td>
<td> Matrix Factorization </td>
<td><a href="https://arxiv.org/abs/0706.4138"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Online Matrix Factorization </td>
<td> Matrix Factorization  </td>
<td><a href="https://www.di.ens.fr/~fbach/mairal10a.pdf"> [arXiv]</td>
</tr>

<tr>
<td> Big data is Low rank </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1705.07474"> [arXiv]</td>
</tr>

<tr>
<td> Geometry of regularization </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1806.09810"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Latent variable model selection </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1008.1290"> [arXiv]</td>
</tr>


<tr>
<td> Wasserstein and Regularization in ML </td>
<td> DRO </td>
<td><a href="https://arxiv.org/abs/1908.08729"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Regularized Risk in DL </td>
<td> DRO </td>
<td><a href="https://arxiv.org/pdf/2109.06294"> [arXiv]</td>
</tr>


<tr>
<td> Implicit Regularization Towards Rank Minimization
in ReLU Networks </td>
<td> Implicit regularization</td>
<td><a href="https://arxiv.org/abs/2201.12760"> [arXiv]</td>
</tr>

<tr>
<td> Limitations of Implicit Bias in Matrix Sensing.</td>
<td> Implicit regularization</td>
<td><a href="https://arxiv.org/abs/2008.12091"> [arXiv]</td>
</tr>

<tr>
<td> Implicit Regularization in Deep Matrix Factorization </td>
<td> Implicit regularization </td>
<td><a href="https://oar.princeton.edu/bitstream/88435/pr1qs0p/1/ImplicitRegularizationDeepMatrixFactor.pdf"> [arXiv]</td>
</tr>

<tr>
<td> Implicit Regularization in Tensor Factorization </td>
<td> Implicit regularization </td>
<td><a href="https://arxiv.org/pdf/2102.09972"> [arXiv]</td>
</tr>

<tr>
<td> Matrix factorization geodesic convexity </td>
<td> Matrix Factorization </td>
<td><a href="https://arxiv.org/abs/2209.15130"> [arXiv]</td>
</tr>


<tr>
<td> Introducción </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/pdf/2408.04809"> [arXiv]</td>
</tr>

<tr>
<td> Aproximación con neural networks </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/2012.14501"> [arXiv], <a href="https://arxiv.org/abs/2307.15772"> [arXiv]</td>
</tr>

<tr>
<td> Espacios de aproximación </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/1905.01208"> [arXiv]</td>
</tr>

<tr>
<td> (FOCM) Theory-to-Practice Gap en DL </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/2104.02746"> [arXiv]</td>
</tr>

<tr>
<td> Universality of transformers </td>
<td> DRO</td>
<td><a href="https://arxiv.org/pdf/2408.01367"> [arXiv]</td>
</tr>

<tr>
<td> Conformal Prediction </td>
<td> </td>
<td><a href="https://www.stat.berkeley.edu/~ryantibs/statlearn-s23/lectures/conformal.pdf"> [notas]</a></td>
</tr>

<tr>
<td> Deep K-SVD Denoising </td>
<td> </td>
<td><a href="https://arxiv.org/abs/1909.13164"> [arXiv]</a></td>
</tr>
  
</table>
</center>

