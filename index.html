<html>
<meta charset="utf-8">
</head><title>Seminario:  2024 -2 </title></head>
<body bgcolor=white text=black link=#FF6600 alink=#0099CC 
vlink=#0099CC><font face="Trebuchet MS"><center>

<font size="10"> <b>Seminario: Optimización y aprendizaje automático</b><br> <p></p> 

<font size="5"> </center> 

<p>El objetivo del seminario es discutir algunos puntos de vista vigentes sobre la relación entre modelos de optimización regularizada (tipo LASSO) y la teoría matemática de aprendizaje automático.</p>

<p><b>HORARIO: Jueves 14.30-16.00 en [, FIng]</b></p>



<center>
<p><b> CALENDARIO DEL SEMINARIO: </b></p>
<table border="1">
<tr>
<td> Titulo </td>
<td> Tema </td>
<td> Conferencista </td>
<td> Links</td>
<td> </td>
</tr>

<tr>
<td> Métrica de Wasserstein y machine learning distribucionalmente robusto (DRO)</td>
<td> DRO </td>
<td> M.Velasco </td>
<td><a href="https://arxiv.org/abs/1908.08729"> [arXiv]</td>
<td></td>
</tr>

<tr>
<td> TBA</td>
<td>  </td>
<td> M.Fiori </td>
<td></td>
<td></td>
</tr>

<tr>
<td> TBA</td>
<td>  </td>
<td> P.Raigorodsky</td>
<td></td>
<td></td>
</tr>

</table>
</center>


<p>Abajo hay algunas referencias posibles de interés (pero charlas de un tema distinto, en un área afin a la descripción del seminario estan bienvenidas!)</p>

<center>
<p><b>Referencias de interes:</b></p>
<table border="1">
<tr>
<td> Titulo </td>
<td> Tema </td>
<td> Link</td>
<td> </td>
</tr>


<tr>
<td> Online Matrix Factorization </td>
<td> Matrix Factorization  </td>
<td><a href="https://www.di.ens.fr/~fbach/mairal10a.pdf"> [arXiv]</td>
</tr>

<tr>
<td> Big data is Low rank </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1705.07474"> [arXiv]</td>
</tr>


<tr>
<td> Rank Minimization (RM) via nuclear norm regularization </td>
<td> Matrix Factorization </td>
<td><a href="https://arxiv.org/abs/0706.4138"> [arXiv]</td>
</tr>

<tr>
<td> Geometry of regularization </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1806.09810"> [arXiv]</td>
</tr>

<tr>
<td> Latent variable model selection </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1008.1290"> [arXiv]</td>
</tr>



<tr>
<td> Regularized Risk in DL </td>
<td> DRO </td>
<td><a href="https://arxiv.org/pdf/2109.06294"> [arXiv]</td>
</tr>


<tr>
<td> Implicit Regularization Towards Rank Minimization
in ReLU Networks </td>
<td> Implicit regularization</td>
<td><a href="https://arxiv.org/abs/2201.12760"> [arXiv]</td>
</tr>

<tr>
<td> Limitations of Implicit Bias in Matrix Sensing.</td>
<td> Implicit regularization</td>
<td><a href="https://arxiv.org/abs/2008.12091"> [arXiv]</td>
</tr>

<tr>
<td> Implicit Regularization in Deep Matrix Factorization </td>
<td> Implicit regularization </td>
<td><a href="https://oar.princeton.edu/bitstream/88435/pr1qs0p/1/ImplicitRegularizationDeepMatrixFactor.pdf"> [arXiv]</td>
</tr>

<tr>
<td> Matrix factorization geodesic convexity </td>
<td> Matrix Factorization </td>
<td><a href="https://arxiv.org/abs/2209.15130"> [arXiv]</td>
</tr>







<tr>
<td> Introducción </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/pdf/2408.04809"> [arXiv]</td>
</tr>

<tr>
<td> Aproximación con neural networks </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/2012.14501"> [arXiv], <a href="https://arxiv.org/abs/2307.15772"> [arXiv]</td>
</tr>

<tr>
<td> Espacios de aproximación </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/1905.01208"> [arXiv]</td>
</tr>

<tr>
<td> (FOCM) Theory-to-Practice Gap en DL </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/2104.02746"> [arXiv]</td>
</tr>

<tr>
<td> Universality of transformers </td>
<td> DRO</td>
<td><a href="https://arxiv.org/pdf/2408.01367"> [arXiv]</td>
</tr>






</table>
</center>

