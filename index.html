<html>
<meta charset="utf-8">
</head><title>Seminario:  2025 </title></head>
<body bgcolor=white text=black link=#FF6600 alink=#0099CC 
vlink=#0099CC><font face="Trebuchet MS"><center>

<font size="10"> <b>Seminario: Optimización y aprendizaje automático (2025)</b><br> <p></p> 

<font size="5"> </center> 

<p><font size="6">El objetivo del seminario es discutir algunos puntos de vista vigentes sobre la relación entre modelos de optimización regularizada (tipo LASSO) y la teoría matemática de aprendizaje automático (machine learning). El seminario será autocontenido y no asume conocimientos previos ni de optimización ni de ML. </p>

<p> Ver página de 2024 <a href="seminario2024.html"> aquí</a></p>
  
<p><b>HORARIO: Jueves 13.30-15.00 en Salón de Seminarios del IMERL (Salón 101) FIng.</b></p>
  
<center>
<p><b> CALENDARIO DEL SEMINARIO: </b></p>
<table border="1">
<tr>
<td> Fecha </td>  
<td> Titulo </td>
<td> Conferencista </td>
<td> Links</td>
<td> </td>
</tr>

<tr>
<td> 27/2/2025</td>  
<td> CoLiDE: Concomitant Linear DAG Estimation</td>
<td> G. Mateos (U. Rochester) </td>
<td><a href="./CoLiDE_slides_handout.pdf"> [diapositivas]</a><a href="https://openreview.net/pdf?id=fGAIgO75dG"> [paper]</a></td>
<td></td>
</tr>

<tr>
<td> 6/3/2025 </td>     
<td> Mathematical modeling challenges in biomedicine (14hs) </td>
<td> Magnus Fontes (Instituto Roche) </td>
<td></td>
<td></td>
</tr>

<tr>
<td> 13/3/2025 </td>   
<td> Suspendido</td>

<td> </td>
<td></td>
<td></td>
</tr>

<tr>
<td> 20/3/2025 </td>     
<td> Grafos Expansores para Compressive Sensing</td>
<td> Pedro Raigorodsky </td>
<td></td>
<td></td>
</tr>

<tr>
<td> 27/3/2025 </td>     
<td> Cómo aproximar objetos en forma de estrella?</td>
<td> Mauricio Velasco </td>
<td> <a href="./StarBodies.pdf"> [diapositivas]</a> </td>
<td></td>
</tr> 


<tr>
<td> 3/4/2025 </td>     
<td> Suspendido</td>
<td>  </td>
<td></td>
<td></td>
</tr>  


<tr>
<td> 10/4/2025 </td>     
<td> Aprendizaje profundo (deep learning) en esferas</td>
<td> Leandro Bentancur</td>
<td></td>
<td></td>
</tr>  

<tr>
<td> 17/4/2025 </td>     
<td> Turismo</td>
<td>  </td>
<td></td>
<td></td>
</tr>  

<tr>
<td> 24/4/2025 </td>     
<td> Exploración estadística de la hipótesis de la variedad</td>
<td> Bernardo Marenco  </td>
<td><a href="./presentacion_manifold_hypothesis.pdf"> [diapositivas]</a></td>
<td></td>
</tr> 

<tr>
<td> 1/5/2025 </td>     
<td> Día del Trabajador</td>
<td> -  </td>
<td></td>
<td></td>
</tr> 

<tr>
<td> 8/5/2025 </td>     
<td> Higher-Order Newton Methods</td>
<td> Marcelo Fiori  </td>
<td><a href="https://arxiv.org/abs/2311.06374"> [paper]</a></td>
<td></td>
</tr> 


<tr>
<td> 15/5/2025 </td>     
<td> Approximate L0 penalization via SOS</td>
<td> Matías Valdés </td>
<td><a href="./Seminario_OML_valdes.pdf">[diapositivas]</td>
<td></td>
</tr> 

<tr>
<td> 22/5/2025 </td>     
<td> TBA</td>
<td> Federico Carrasco </td>
<td></td>
<td></td>
</tr> 


<tr>
<td> 29/5/2025 </td>     
<td> TBA</td>
<td> </td>
<td></td>
<td></td>
</tr> 


<tr>
<td> 5/6/2025 </td>     
<td> TBA</td>
<td> </td>
<td></td>
<td></td>
</tr> 

  
  
</table>
</center>

<font size="5">
<p>ORGANIZADORES: 
<li>Pedro Raigorodsky (pedro.raigorodsky@gmail.com)</li> 
<li>Marcelo Fiori (mfiori@fing.edu.uy)</li> 
<li>Mauricio Velasco (mauricio.velasco@ucu.edu.uy)</li> 
</p>
<p>Si quiere dar una charla en el Seminario por favor escribir a los organizadores. Abajo hay algunas referencias con temas y artículos de posible interés (pero charlas de un tema distinto, en un área afin a la descripción del seminario estan bienvenidas!)</p>

<center>
<p><b>Referencias de interes:</b></p>
<table border="1">
<tr>
<td> Titulo </td>
<td> Tema </td>
<td> Link</td>
<td> EsClave</td>
</tr>



<tr>
<td> Compressive Sampling </td>
<td> Compressive Sensing  </td>
<td><a href="https://candes.su.domains/publications/downloads/CompressiveSampling.pdf"> [Link] </td>
<td>*</td>
</tr>

<tr>
<td> Compressive Fourier </td>
<td> Compressive Sensing  </td>
<td><a href="https://arxiv.org/pdf/math/0409186"> [arXiv]</td>
<td></td>
</tr>


<tr>
<td> CS on measures </td>
<td> Compressive Sensing  </td>
<td><a href="https://arxiv.org/pdf/1103.4951"> [arXiv]</td>
<td></td>
</tr>



<tr>
<td> Rank Minimization (RM) via nuclear norm regularization </td>
<td> Matrix Factorization </td>
<td><a href="https://arxiv.org/abs/0706.4138"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Online Matrix Factorization </td>
<td> Matrix Factorization  </td>
<td><a href="https://www.di.ens.fr/~fbach/mairal10a.pdf"> [arXiv]</td>
</tr>

<tr>
<td> Big data is Low rank </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1705.07474"> [arXiv]</td>
</tr>

<tr>
<td> Geometry of regularization </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1806.09810"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Latent variable model selection </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1008.1290"> [arXiv]</td>
</tr>


<tr>
<td> Wasserstein and Regularization in ML </td>
<td> DRO </td>
<td><a href="https://arxiv.org/abs/1908.08729"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Regularized Risk in DL </td>
<td> DRO </td>
<td><a href="https://arxiv.org/pdf/2109.06294"> [arXiv]</td>
</tr>


<tr>
<td> Implicit Regularization Towards Rank Minimization
in ReLU Networks </td>
<td> Implicit regularization</td>
<td><a href="https://arxiv.org/abs/2201.12760"> [arXiv]</td>
</tr>

<tr>
<td> Limitations of Implicit Bias in Matrix Sensing.</td>
<td> Implicit regularization</td>
<td><a href="https://arxiv.org/abs/2008.12091"> [arXiv]</td>
</tr>

<tr>
<td> Implicit Regularization in Deep Matrix Factorization </td>
<td> Implicit regularization </td>
<td><a href="https://oar.princeton.edu/bitstream/88435/pr1qs0p/1/ImplicitRegularizationDeepMatrixFactor.pdf"> [arXiv]</td>
</tr>

<tr>
<td> Implicit Regularization in Tensor Factorization </td>
<td> Implicit regularization </td>
<td><a href="https://arxiv.org/pdf/2102.09972"> [arXiv]</td>
</tr>

<tr>
<td> Matrix factorization geodesic convexity </td>
<td> Matrix Factorization </td>
<td><a href="https://arxiv.org/abs/2209.15130"> [arXiv]</td>
</tr>


<tr>
<td> Introducción </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/pdf/2408.04809"> [arXiv]</td>
</tr>

<tr>
<td> Aproximación con neural networks </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/2012.14501"> [arXiv], <a href="https://arxiv.org/abs/2307.15772"> [arXiv]</td>
</tr>

<tr>
<td> Espacios de aproximación </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/1905.01208"> [arXiv]</td>
</tr>

<tr>
<td> (FOCM) Theory-to-Practice Gap en DL </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/2104.02746"> [arXiv]</td>
</tr>

<tr>
<td> Universality of transformers </td>
<td> DRO</td>
<td><a href="https://arxiv.org/pdf/2408.01367"> [arXiv]</td>
</tr>

<tr>
<td> Conformal Prediction </td>
<td> </td>
<td><a href="https://www.stat.berkeley.edu/~ryantibs/statlearn-s23/lectures/conformal.pdf"> [notas]</a></td>
</tr>

<tr>
<td> Deep K-SVD Denoising </td>
<td> </td>
<td><a href="https://arxiv.org/abs/1909.13164"> [arXiv]</a></td>
</tr>
  
</table>
</center>

