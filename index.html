<html>
<meta charset="utf-8">
</head><title>Seminario:  2024 -2 </title></head>
<body bgcolor=white text=black link=#FF6600 alink=#0099CC 
vlink=#0099CC><font face="Trebuchet MS"><center>

<font size="10"> <b>Seminario: Optimización y aprendizaje automático</b><br> <p></p> 

<font size="5"> </center> 

<p><font size="6">El objetivo del seminario es discutir algunos puntos de vista vigentes sobre la relación entre modelos de optimización regularizada (tipo LASSO) y la teoría matemática de aprendizaje automático (machine learning). El seminario será autocontenido y no asume conocimientos previos ni de optimización ni de ML. </p>

<p><b>HORARIO: Jueves 14.30-16.00 en Salón 723 FIng.</b></p>
  
<center>
<p><b> CALENDARIO DEL SEMINARIO: </b></p>
<table border="1">
<tr>
<td> Fecha </td>  
<td> Titulo </td>
<td> Tema </td>
<td> Conferencista </td>
<td> Links</td>
<td> </td>
</tr>

<tr>
<td> 29/8 </td>  
<td> Métrica de Wasserstein y machine learning distribucionalmente robusto (DRO)</td>
<td> DRO </td>
<td> M.Velasco </td>
<td><a href="https://arxiv.org/abs/1908.08729"> [arXiv],<a href="https://www.youtube.com/watch?v=piYnd_wYlT8"> [Video]</td>
<td></td>
</tr>

<tr>
<td> 19/9 </td>     
<td> Suspendido </td>
<td>  </td>
<td> </td>
<td></td>
<td></td>
</tr>

<tr>
<td> 12/9 </td>   
<td> TBA</td>
<td>  </td>
<td> P.Raigorodsky</td>
<td></td>
<td></td>
</tr>

<tr>
<td> 19/9 </td>     
<td> TBA</td>
<td>  </td>
<td> M.Fiori </td>
<td></td>
<td></td>
</tr>

</table>
</center>

<font size="5">
<p>ORGANIZADORES: 
<li>Pedro Raigorodsky (pedro.raigorodsky@gmail.com)</li> 
<li>Marcelo Fiori (mfiori@fing.edu.uy)</li> 
<li>Mauricio Velasco (mauricio.velasco@ucu.edu.uy)</li> 
</p>
<p>Si quiere dar una charla en el Seminario por favor escribir a los organizadores. Abajo hay algunas referencias con temas y artículos de posible interés (pero charlas de un tema distinto, en un área afin a la descripción del seminario estan bienvenidas!)</p>

<center>
<p><b>Referencias de interes:</b></p>
<table border="1">
<tr>
<td> Titulo </td>
<td> Tema </td>
<td> Link</td>
<td> EsClave</td>
</tr>



<tr>
<td> Compressive Sampling </td>
<td> Compressive Sensing  </td>
<td><a href="https://candes.su.domains/publications/downloads/CompressiveSampling.pdf"> [Link] </td>
<td>*</td>
</tr>

<tr>
<td> Compressive Fourier </td>
<td> Compressive Sensing  </td>
<td><a href="https://arxiv.org/pdf/math/0409186"> [arXiv]</td>
<td></td>
</tr>


<tr>
<td> CS on measures </td>
<td> Compressive Sensing  </td>
<td><a href="https://arxiv.org/pdf/1103.4951"> [arXiv]</td>
<td></td>
</tr>



<tr>
<td> Rank Minimization (RM) via nuclear norm regularization </td>
<td> Matrix Factorization </td>
<td><a href="https://arxiv.org/abs/0706.4138"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Online Matrix Factorization </td>
<td> Matrix Factorization  </td>
<td><a href="https://www.di.ens.fr/~fbach/mairal10a.pdf"> [arXiv]</td>
</tr>

<tr>
<td> Big data is Low rank </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1705.07474"> [arXiv]</td>
</tr>

<tr>
<td> Geometry of regularization </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1806.09810"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Latent variable model selection </td>
<td> Matrix Factorization  </td>
<td><a href="https://arxiv.org/abs/1008.1290"> [arXiv]</td>
</tr>


<tr>
<td> Wasserstein and Regularization in ML </td>
<td> DRO </td>
<td><a href="https://arxiv.org/abs/1908.08729"> [arXiv]</td>
<td>*</td>
</tr>

<tr>
<td> Regularized Risk in DL </td>
<td> DRO </td>
<td><a href="https://arxiv.org/pdf/2109.06294"> [arXiv]</td>
</tr>


<tr>
<td> Implicit Regularization Towards Rank Minimization
in ReLU Networks </td>
<td> Implicit regularization</td>
<td><a href="https://arxiv.org/abs/2201.12760"> [arXiv]</td>
</tr>

<tr>
<td> Limitations of Implicit Bias in Matrix Sensing.</td>
<td> Implicit regularization</td>
<td><a href="https://arxiv.org/abs/2008.12091"> [arXiv]</td>
</tr>

<tr>
<td> Implicit Regularization in Deep Matrix Factorization </td>
<td> Implicit regularization </td>
<td><a href="https://oar.princeton.edu/bitstream/88435/pr1qs0p/1/ImplicitRegularizationDeepMatrixFactor.pdf"> [arXiv]</td>
</tr>

<tr>
<td> Implicit Regularization in Tensor Factorization </td>
<td> Implicit regularization </td>
<td><a href="https://arxiv.org/pdf/2102.09972"> [arXiv]</td>
</tr>

<tr>
<td> Matrix factorization geodesic convexity </td>
<td> Matrix Factorization </td>
<td><a href="https://arxiv.org/abs/2209.15130"> [arXiv]</td>
</tr>


<tr>
<td> Introducción </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/pdf/2408.04809"> [arXiv]</td>
</tr>

<tr>
<td> Aproximación con neural networks </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/2012.14501"> [arXiv], <a href="https://arxiv.org/abs/2307.15772"> [arXiv]</td>
</tr>

<tr>
<td> Espacios de aproximación </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/1905.01208"> [arXiv]</td>
</tr>

<tr>
<td> (FOCM) Theory-to-Practice Gap en DL </td>
<td> Geometría de Deep Learning</td>
<td><a href="https://arxiv.org/abs/2104.02746"> [arXiv]</td>
</tr>

<tr>
<td> Universality of transformers </td>
<td> DRO</td>
<td><a href="https://arxiv.org/pdf/2408.01367"> [arXiv]</td>
</tr>

</table>
</center>

